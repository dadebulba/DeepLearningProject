{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# import necessary libraries\r\n",
    "import torch\r\n",
    "import torchvision\r\n",
    "import torch.nn.functional as F\r\n",
    "import torchvision.transforms as T\r\n",
    "import pandas as pd\r\n",
    "from skimage import io, transform\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from torchvision import transforms, utils\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "import os\r\n",
    "from os import listdir\r\n",
    "from os.path import isfile, join\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "# print cuda info\r\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\r\n",
    "print(f\"Cuda device count: {torch.cuda.device_count()}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cuda available: True\n",
      "Cuda device count: 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def setupLabelsDict(annotations_frame):\r\n",
    "   \r\n",
    "    labels = {}\r\n",
    "    index = 0\r\n",
    "    for i in list(annotations_frame):\r\n",
    "        if(i != \"id\"):\r\n",
    "            for j in range(min(annotations_frame[i]), max(annotations_frame[i])+1):\r\n",
    "                labels[f\"{i}-{j}\"] = index\r\n",
    "                index+=1\r\n",
    "    return labels\r\n",
    "\r\n",
    "def getTargetEncoding(id, annotations_frame, labels):\r\n",
    "    encoding = [0 for _ in range(len(labels))]\r\n",
    "    labels_df = annotations_frame.loc[annotations_frame['id'] == id]\r\n",
    "    for label, content in labels_df.items():\r\n",
    "        if(label != 'id'):\r\n",
    "            encoding[labels[\"%s-%s\" % (label, labels_df[label].iloc[0])]] += 1\r\n",
    "    return encoding\r\n",
    "\r\n",
    "def convertAnnotationsFrame(annotations_frame, train_dir):\r\n",
    "    annotations_frame = pd.read_csv('dataset/annotations_train.csv')\r\n",
    "\r\n",
    "    img_files = [f for f in listdir(train_dir)]\r\n",
    "\r\n",
    "    augmented_annotations_list = [] \r\n",
    "    for entry in annotations_frame.itertuples():\r\n",
    "        for i in img_files:\r\n",
    "            if(int(entry[1]) == int(i.split(\"_\")[0])):\r\n",
    "                img_with_annotation = {\r\n",
    "                    \"id\": i, \r\n",
    "                    \"age\": entry[2], \r\n",
    "                    \"backpack\":entry[3],\r\n",
    "                    \"bag\":entry[4],\r\n",
    "                    \"handbag\":entry[5],\r\n",
    "                    \"clothes\":entry[6],\r\n",
    "                    \"down\":entry[7],\r\n",
    "                    \"up\":entry[8],\r\n",
    "                    \"hair\":entry[9],\r\n",
    "                    \"hat\":entry[10],\r\n",
    "                    \"gender\":entry[11],\r\n",
    "                    \"upblack\":entry[12],\r\n",
    "                    \"upwhite\":entry[13],\r\n",
    "                    \"upred\":entry[14],\r\n",
    "                    \"uppurple\":entry[15],\r\n",
    "                    \"upyellow\":entry[16],\r\n",
    "                    \"upgray\":entry[17],\r\n",
    "                    \"upblue\":entry[18],\r\n",
    "                    \"upgreen\":entry[19],\r\n",
    "                    \"downblack\":entry[20],\r\n",
    "                    \"downwhite\":entry[21],\r\n",
    "                    \"downpink\":entry[22],\r\n",
    "                    \"downpurple\":entry[23],\r\n",
    "                    \"downyellow\":entry[24],\r\n",
    "                    \"downgray\":entry[25],\r\n",
    "                    \"downblue\":entry[26],\r\n",
    "                    \"downgreen\":entry[27],\r\n",
    "                    \"downbrown\":entry[28]\r\n",
    "                }\r\n",
    "                augmented_annotations_list.append(img_with_annotation)\r\n",
    "\r\n",
    "    augmented_annotations_frame = pd.DataFrame(augmented_annotations_list)\r\n",
    "    return augmented_annotations_frame\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "class PeopleDataset(Dataset):\r\n",
    "    \"\"\"People with annotations dataset.\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, frame_with_labels, root_dir, labels, train, transform=None):\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            csv_file (string): Path to the csv file with annotations.\r\n",
    "            root_dir (string): Directory with all the images.\r\n",
    "            transform (callable, optional): Optional transform to be applied\r\n",
    "                on a sample.\r\n",
    "        \"\"\"\r\n",
    "        self.annotations_frame = frame_with_labels\r\n",
    "        self.root_dir = root_dir\r\n",
    "        self.transform = transform\r\n",
    "        self.img_files = [f for f in listdir(root_dir)]\r\n",
    "        self.labels = labels\r\n",
    "        self.train = train\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.img_files)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        if torch.is_tensor(idx):\r\n",
    "            idx = idx.tolist()\r\n",
    "\r\n",
    "        if self.train:\r\n",
    "            img_name = os.path.join(self.root_dir,self.annotations_frame.iloc[idx, 0])\r\n",
    "            image = io.imread(img_name)\r\n",
    "            image = T.ToTensor()(image)\r\n",
    "            image = F.interpolate(image, size=128)  \r\n",
    "            encoding = getTargetEncoding(self.annotations_frame.iloc[idx, 0],self.annotations_frame, self.labels)\r\n",
    "            sample = (image, torch.tensor(encoding))\r\n",
    "            return sample\r\n",
    "        else:\r\n",
    "            image = io.imread(\"%s/%s\" % (self.root_dir, self.img_files[idx]))\r\n",
    "            image = T.ToTensor()(image)\r\n",
    "            image = F.interpolate(image, size=128)  \r\n",
    "            sample = image\r\n",
    "            return sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "'''\r\n",
    "Input arguments\r\n",
    "  num_classes: number of classes in the dataset.\r\n",
    "               This is equal to the number of output neurons.\r\n",
    "'''\r\n",
    "\r\n",
    "def initialize_alexnet(num_classes):\r\n",
    "  # load the pre-trained Alexnet\r\n",
    "  alexnet = torchvision.models.alexnet(pretrained=True)\r\n",
    "  \r\n",
    "  # get the number of neurons in the penultimate layer\r\n",
    "  in_features = alexnet.classifier[6].in_features\r\n",
    "  \r\n",
    "  # re-initalize the output layer\r\n",
    "  alexnet.classifier[6] = torch.nn.Sequential(\r\n",
    "    torch.nn.Linear(in_features=in_features, out_features=num_classes),\r\n",
    "    torch.nn.Sigmoid()\r\n",
    "  )\r\n",
    "  return alexnet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def get_data(labels, batch_size, img_root, test_batch_size=256):\r\n",
    "  \r\n",
    "  # Prepare data transformations and then combine them sequentially\r\n",
    "  # transform = list()\r\n",
    "  # transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\r\n",
    "  # transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\r\n",
    "  # transform = T.Compose(transform)                          # Composes the above transformations into one.\r\n",
    "\r\n",
    "  # Load data\r\n",
    "  test_data = PeopleDataset(frame_with_labels=None,\r\n",
    "                                      root_dir=\"%s/test\" % (img_root),\r\n",
    "                                      labels=labels,\r\n",
    "                                      train=False)\r\n",
    "  \r\n",
    "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=0) #before num_workers=4\r\n",
    "  \r\n",
    "  return test_loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def test(net, data_loader, device='cuda:0'):\r\n",
    "  samples = 0.\r\n",
    "  cumulative_loss = 0.\r\n",
    "  cumulative_accuracy = 0.\r\n",
    "\r\n",
    "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
    "  with torch.no_grad():\r\n",
    "    for batch_idx, inputs in enumerate(data_loader):\r\n",
    "      print(batch_idx)\r\n",
    "      # Load data into GPU\r\n",
    "      inputs = inputs.to(device)\r\n",
    "      \r\n",
    "      outputs = net(inputs)\r\n",
    "      #print(outputs)\r\n",
    "      # Apply the loss\r\n",
    "      predicted = torch.round(outputs)\r\n",
    "      print(predicted)\r\n",
    "  return predicted\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def get_cost_function():\r\n",
    "  cost_function = torch.nn.BCELoss()\r\n",
    "  return cost_function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "def main(device='cuda:0', \r\n",
    "         img_root='./dataset',\r\n",
    "         batch_size=128):\r\n",
    "  \r\n",
    "  writer = SummaryWriter(log_dir=\"runs/exp1\")\r\n",
    "\r\n",
    "  annotations_frame = pd.read_csv(\"./dataset/annotations_train.csv\")\r\n",
    "  augmented_annotations_frame = convertAnnotationsFrame(annotations_frame, \"%s/train\" % (img_root))\r\n",
    "  labels = setupLabelsDict(augmented_annotations_frame)\r\n",
    "  num_classes = len(labels)\r\n",
    "  # Instantiates dataloaders\r\n",
    "  test_loader = get_data(labels=labels, batch_size=batch_size, img_root=img_root)\r\n",
    "  \r\n",
    "  # Instantiates the model\r\n",
    "  net = initialize_alexnet(num_classes)\r\n",
    "  net.load_state_dict(torch.load(\"./model.pth\"))\r\n",
    "  net.to(device)\r\n",
    "  print('Before training:')\r\n",
    "  test_res = test(net, test_loader)\r\n",
    "  print(\"Test result: %s \" % (test_res))\r\n",
    "  print('-----------------------------------------------------')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before training:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}