{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    },
    "colab": {
      "name": "ModelEvaluation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/ModelEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8xqOqksOdpP"
      },
      "source": [
        "# Deep Learning Project - Person classification task evaluation\n",
        "\n",
        "[https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/ModelEvaluation.ipynb](https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/ModelEvaluation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPhx7YfjOlVP"
      },
      "source": [
        "Importing from Google Drive the dataset.zip and extract into dataset folder, change the path with your dataset location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyNEBS-wCrYN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/MyDrive/UNITN/5° anno/Deep Learning 2021/dataset.zip\" -d dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YZ4yKp2OmmI"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKoDlK5mByA6"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import csv\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# print cuda info\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSIUNI37ByA9"
      },
      "source": [
        "'''\n",
        "Returns: dict where keys are the labels associated with their encoding and the values are the corresponding indexes in the one-hot encoding\n",
        "Input arguments\n",
        "  annotations_frame: dataframe containing annotations associated to people ids\n",
        "'''\n",
        "def setupLabelsDict(annotations_frame):\n",
        "    labels = {}\n",
        "    index = 0\n",
        "    for i in list(annotations_frame):\n",
        "        # id is not part of labels\n",
        "        if(i != \"id\"):\n",
        "            # to build the one-hot encoding I need to know the min and max value range for each label\n",
        "            for j in range(min(annotations_frame[i]), max(annotations_frame[i])+1):\n",
        "                labels[f\"{i}-{j}\"] = index\n",
        "                index+=1\n",
        "    return labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmn4iZz2ByA-"
      },
      "source": [
        "class PeopleTestDataset(Dataset):\n",
        "    \"\"\"People test dataset containing only images.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Directory with all the images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.img_files = [f for f in listdir(root_dir)]\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = io.imread(\"%s/%s\" % (self.root_dir, self.img_files[idx]))\n",
        "        image = T.ToTensor()(image)\n",
        "        image = F.interpolate(image, size=128)  \n",
        "        return (image, self.img_files[idx]) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_EX1FZLByA_"
      },
      "source": [
        "'''\n",
        "Input arguments\n",
        "  num_classes: number of classes in the dataset.\n",
        "               This is equal to the number of output neurons.\n",
        "'''\n",
        "\n",
        "def initialize_resnet(num_classes):\n",
        "  # load the pre-trained ResNet-50\n",
        "  resnet = torchvision.models.resnet50(pretrained=True, progress=False)\n",
        "  num_features = resnet.fc.in_features\n",
        "  resnet.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=num_features, out_features=1024),\n",
        "    torch.nn.Linear(in_features=1024, out_features=512),\n",
        "    torch.nn.Linear(in_features=512, out_features=num_classes),\n",
        "    torch.nn.Sigmoid()\n",
        "  )\n",
        "  return resnet"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UL5YJobByA_"
      },
      "source": [
        "\"\"\"\n",
        "Returns: \n",
        "  test_loader: DataLoader used during testing\n",
        "Input arguments\n",
        "  img_root: directory containing the dataset images\n",
        "  test_batch_size: batch size used during validation and test phase\n",
        "\"\"\"\n",
        "def get_data(img_root, test_batch_size=1):\n",
        "  # Load data\n",
        "  test_data = PeopleTestDataset(root_dir=img_root)\n",
        "  \n",
        "  # Instantiate DataLoader\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=0)\n",
        "  \n",
        "  return test_loader"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gIgYIHCByBA"
      },
      "source": [
        "\"\"\"\n",
        "Returns: predictions for test images\n",
        "Input arguments\n",
        "  net: trained network\n",
        "  data_loader: DataLoader containing test data\n",
        "  device: which device to use during testing phase (default is GPU)\n",
        "\"\"\"\n",
        "def test(net, data_loader, device='cuda:0'):\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  predictions = {}\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, img_file) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "      \n",
        "      # Save predictions\n",
        "      predicted = torch.round(outputs)\n",
        "      predictions[img_file] = predicted\n",
        "\n",
        "  return predictions\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECxjNqhuGQLJ"
      },
      "source": [
        "\"\"\"\n",
        "Returns: refined prediction with format based on origina annotations dataframe\n",
        "Input arguments\n",
        "  labels: computed labels\n",
        "  test_res: test result in a one-hot encoding format\n",
        "\"\"\"\n",
        "def refine_predictions(labels, test_res):\n",
        "  predictions_refined = []\n",
        "  # convert list of \"label-value\" to only list of \"label\"\n",
        "  single_labels = list(set([el.split(\"-\")[0] for el in labels.keys()]))\n",
        "  for img in test_res:\n",
        "    pred = list(np.concatenate(test_res[img].tolist()).flat)\n",
        "    current_pred = {}\n",
        "    current_pred[\"id\"] = img[0].split(\".\")[0] \n",
        "    for l in single_labels:\n",
        "      current_pred[l] = 1\n",
        "    for key in labels:\n",
        "      if (pred[labels[key]] == 1):\n",
        "        current_pred[key.split(\"-\")[0]] = key.split(\"-\")[1]\n",
        "    predictions_refined.append(current_pred)\n",
        "  return predictions_refined\n",
        "\n",
        "\"\"\"\n",
        "Input arguments\n",
        "  annotations_frame: origina annotations dataframe\n",
        "  predictions_refined: refined prediction converted from the one-hot encoding\n",
        "\"\"\"\n",
        "def write_results_csv(annotations_frame, predictions_refined):\n",
        "  with open('results.csv', 'w',  newline='') as f:\n",
        "      # create the csv writer\n",
        "      writer = csv.writer(f)\n",
        "      # write the header\n",
        "      writer.writerow(list(annotations_frame.keys()))\n",
        "      \n",
        "      for pred in predictions_refined:\n",
        "        row = []\n",
        "        for key in annotations_frame.keys():\n",
        "          row.append(pred[key])\n",
        "        writer.writerow(row)  \n",
        "      "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehjpVoJfByBC"
      },
      "source": [
        "def main(device='cuda:0', \n",
        "         img_root='./dataset',\n",
        "         model_location=\"/content/drive/MyDrive/UNITN/5° anno/Deep Learning 2021/models/resnet50_5epoch.pth\",\n",
        "         batch_size=128):\n",
        "  \n",
        "  # Load csv with annotations into dataframe\n",
        "  annotations_frame = pd.read_csv(\"%s/annotations_train.csv\" % (img_root))\n",
        "\n",
        "  # Get labels dict\n",
        "  labels = setupLabelsDict(annotations_frame)\n",
        "  num_classes = len(labels)\n",
        "\n",
        "  # Instantiates dataloaders\n",
        "  test_loader = get_data(img_root=\"%s/test\" % (img_root))\n",
        "  \n",
        "  # Instantiates the model to evaluate\n",
        "  net = initialize_resnet(num_classes)\n",
        "  net.load_state_dict(torch.load(model_location))\n",
        "  net.to(device)\n",
        "\n",
        "  # Get test dataset predictions\n",
        "  test_res = test(net, test_loader)\n",
        "\n",
        "  # Get predictions from the one-hot encoding from the original label\n",
        "  predictions_refined = refine_predictions(labels, test_res)\n",
        "\n",
        "  # Write the prediction result to CSV\n",
        "  write_results_csv(annotations_frame, predictions_refined)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H28JLzqpByBD"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}