{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4h0F8jKyF6wPwdT4Qef6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepLearning project"
      ],
      "metadata": {
        "id": "xvScpF6b8OzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# import necessary libraries\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "\r\n",
        "# print cuda info\r\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\r\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")\r\n",
        "print(f\"Cuda device used: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available: True\n",
            "Cuda device count: 1\n",
            "Cuda device used: GeForce RTX 2060 SUPER\n"
          ]
        }
      ],
      "metadata": {
        "id": "Ft1ZGWBU8OQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing\r\n",
        "Make sure to extract the dataset zip into the 'dataset' folder"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Network"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_cost_function():\r\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\r\n",
        "  return cost_function"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\r\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\r\n",
        "  return optimizer"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  with torch.no_grad():\r\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\r\n",
        "      # Load data into GPU\r\n",
        "      inputs = inputs.to(device)\r\n",
        "      targets = targets.to(device)\r\n",
        "        \r\n",
        "      # Forward pass\r\n",
        "      outputs = net(inputs)\r\n",
        "\r\n",
        "      # Apply the loss\r\n",
        "      loss = cost_function(outputs, targets)\r\n",
        "\r\n",
        "      # Better print something\r\n",
        "      samples+=inputs.shape[0]\r\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\r\n",
        "      _, predicted = outputs.max(1)\r\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\r\n",
        "\r\n",
        "\r\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  \r\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\r\n",
        "    # Load data into GPU\r\n",
        "    inputs = inputs.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "      \r\n",
        "    # Forward pass\r\n",
        "    outputs = net(inputs)\r\n",
        "\r\n",
        "    # Apply the loss\r\n",
        "    loss = cost_function(outputs,targets)\r\n",
        "      \r\n",
        "    # Backward pass\r\n",
        "    loss.backward()\r\n",
        "    \r\n",
        "    # Update parameters\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    # Reset the optimizer\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # Better print something, no?\r\n",
        "    samples+=inputs.shape[0]\r\n",
        "    cumulative_loss += loss.item()\r\n",
        "    _, predicted = outputs.max(1)\r\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_data(batch_size, test_batch_size=256):\r\n",
        "  \r\n",
        "  # Prepare data transformations and then combine them sequentially\r\n",
        "  transform = list()\r\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\r\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\r\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\r\n",
        "\r\n",
        "  # Load data\r\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True) \r\n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True) \r\n",
        "  \r\n",
        "\r\n",
        "  # Create train and validation splits\r\n",
        "  num_samples = len(full_training_data)\r\n",
        "  training_samples = int(num_samples*0.5+1)\r\n",
        "  validation_samples = num_samples - training_samples\r\n",
        "\r\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\r\n",
        "  \r\n",
        "  # Initialize dataloaders\r\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=8)       #before num_workers=4\r\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=8) #before num_workers=4\r\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=8) #before num_workers=4\r\n",
        "  \r\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''\r\n",
        "Input arguments\r\n",
        "  batch_size: Size of a mini-batch\r\n",
        "  device: GPU where you want to train your network\r\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\r\n",
        "  momentum: Momentum for SGD optimizer\r\n",
        "  epochs: Number of epochs for training the network\r\n",
        "'''\r\n",
        "\r\n",
        "def main(batch_size=1024, \r\n",
        "         device='cuda:0', \r\n",
        "         learning_rate=0.01, \r\n",
        "         weight_decay=0.000001, \r\n",
        "         momentum=0.9, \r\n",
        "         epochs=25):\r\n",
        "  \r\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\r\n",
        "  \r\n",
        "  net =  LeNet().to(device)\r\n",
        "  \r\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\r\n",
        "  \r\n",
        "  cost_function = get_cost_function()\r\n",
        "\r\n",
        "  print('Before training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')\r\n",
        "\r\n",
        "  for e in range(epochs):\r\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\r\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "    print('Epoch: {:d}'.format(e+1))\r\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "    print('-----------------------------------------------------')\r\n",
        "  print('After training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Free GPU memory\r\n",
        "torch.cuda.empty_cache()\r\n",
        "import gc\r\n",
        "gc.collect()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "main()"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}