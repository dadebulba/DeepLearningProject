{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4h0F8jKyF6wPwdT4Qef6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb](https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Project"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# import necessary libraries\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import pandas as pd\r\n",
        "from skimage import io, transform\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision import transforms, utils\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import os\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "\r\n",
        "# print cuda info\r\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\r\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")\r\n",
        "print(f\"Cuda device used: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\r\n",
        "\r\n",
        "# constants\r\n",
        "DIR_DATASET_TRAIN = \"./dataset/train\""
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available: True\n",
            "Cuda device count: 1\n",
            "Cuda device used: GeForce RTX 2060 SUPER\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing\r\n",
        "Make sure to extract the zip into the 'dataset' folder"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "annotations_frame = pd.read_csv('dataset/annotations_train.csv')\r\n",
        "print(annotations_frame.iloc[0, 0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "474\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def setupLabelsDict(annotations_frame):\r\n",
        "   \r\n",
        "    labels = {}\r\n",
        "    index = 0\r\n",
        "    for i in list(annotations_frame):\r\n",
        "        if(i != \"id\"):\r\n",
        "            for j in range(min(annotations_frame[i]), max(annotations_frame[i])+1):\r\n",
        "                labels[f\"{i}-{j}\"] = index\r\n",
        "                index+=1\r\n",
        "    return labels\r\n",
        "\r\n",
        "def getTargetEncoding(id, annotations_frame, labels):\r\n",
        "    print(\"Get target encoding triggered\")\r\n",
        "    encoding = [0 for _ in range(len(labels))]\r\n",
        "    labels_df = annotations_frame.loc[annotations_frame['id'] == id]\r\n",
        "    for label, content in labels_df.items():\r\n",
        "        if(label != 'id'):\r\n",
        "            encoding[labels[(f\"{label}-{labels_df[label].iloc[0]}\")]] += 1\r\n",
        "    return encoding"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "class PeopleDataset(Dataset):\r\n",
        "    \"\"\"People with annotations dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, frame_with_labels, root_dir, labels, transform=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.annotations_frame = frame_with_labels\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.transform = transform\r\n",
        "        self.img_files = [f for f in listdir(DIR_DATASET_TRAIN)]\r\n",
        "        self.labels = labels\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.annotations_frame)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        img_name = os.path.join(self.root_dir,self.annotations_frame.iloc[idx, 0])\r\n",
        "        image = io.imread(img_name)\r\n",
        "        encoding = getTargetEncoding(self.annotations_frame.iloc[idx, 0],self.annotations_frame, labels)\r\n",
        "        sample = {'image': image, 'annotations': encoding}\r\n",
        "\r\n",
        "        return sample"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "annotations_frame = pd.read_csv('dataset/annotations_train.csv')\r\n",
        "\r\n",
        "img_files = [f for f in listdir(DIR_DATASET_TRAIN)]\r\n",
        "\r\n",
        "augmented_annotations_list = [] \r\n",
        "for entry in annotations_frame.itertuples():\r\n",
        "    for i in img_files:\r\n",
        "        if(int(entry[1]) == int(i.split(\"_\")[0])):\r\n",
        "            img_with_annotation = {\r\n",
        "                \"id\": i, \r\n",
        "                \"age\": entry[2], \r\n",
        "                \"backpack\":entry[3],\r\n",
        "                \"bag\":entry[4],\r\n",
        "                \"handbag\":entry[5],\r\n",
        "                \"clothes\":entry[6],\r\n",
        "                \"down\":entry[7],\r\n",
        "                \"up\":entry[8],\r\n",
        "                \"hair\":entry[9],\r\n",
        "                \"hat\":entry[10],\r\n",
        "                \"gender\":entry[11],\r\n",
        "                \"upblack\":entry[12],\r\n",
        "                \"upwhite\":entry[13],\r\n",
        "                \"upred\":entry[14],\r\n",
        "                \"uppurple\":entry[15],\r\n",
        "                \"upyellow\":entry[16],\r\n",
        "                \"upgray\":entry[17],\r\n",
        "                \"upblue\":entry[18],\r\n",
        "                \"upgreen\":entry[19],\r\n",
        "                \"downblack\":entry[20],\r\n",
        "                \"downwhite\":entry[21],\r\n",
        "                \"downpink\":entry[22],\r\n",
        "                \"downpurple\":entry[23],\r\n",
        "                \"downyellow\":entry[24],\r\n",
        "                \"downgray\":entry[25],\r\n",
        "                \"downblue\":entry[26],\r\n",
        "                \"downgreen\":entry[27],\r\n",
        "                \"downbrown\":entry[28]\r\n",
        "            }\r\n",
        "            augmented_annotations_list.append(img_with_annotation)\r\n",
        "\r\n",
        "augmented_annotations_frame = pd.DataFrame(augmented_annotations_list)\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "labels = setupLabelsDict(annotations_frame)\r\n",
        "people_dataset = PeopleDataset(frame_with_labels=augmented_annotations_frame,\r\n",
        "                                    root_dir='./dataset/train',\r\n",
        "                                    labels=labels)\r\n",
        "\r\n",
        "print(\"Dataset Initialized\")\r\n",
        "dataloader = DataLoader(people_dataset, batch_size=4,\r\n",
        "                        shuffle=True, num_workers=1)\r\n",
        "print(\"DataLoader Initialized\")\r\n",
        "print(len(people_dataset))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Initialized\n",
            "DataLoader Initialized\n",
            "12989\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# WARNING: this will cause infinite run time\r\n",
        "for batch_idx, (inputs, targets) in enumerate(dataloader):\r\n",
        "    print(i_batch, len(inputs),len(targets))\r\n",
        "    if i_batch == 3:\r\n",
        "        break"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Network"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_cost_function():\r\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\r\n",
        "  return cost_function"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\r\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\r\n",
        "  return optimizer"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  with torch.no_grad():\r\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\r\n",
        "      # Load data into GPU\r\n",
        "      inputs = inputs.to(device)\r\n",
        "      targets = targets.to(device)\r\n",
        "        \r\n",
        "      # Forward pass\r\n",
        "      outputs = net(inputs)\r\n",
        "\r\n",
        "      # Apply the loss\r\n",
        "      loss = cost_function(outputs, targets)\r\n",
        "\r\n",
        "      # Better print something\r\n",
        "      samples+=inputs.shape[0]\r\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\r\n",
        "      _, predicted = outputs.max(1)\r\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\r\n",
        "\r\n",
        "\r\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  \r\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\r\n",
        "    # Load data into GPU\r\n",
        "    inputs = inputs.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "      \r\n",
        "    # Forward pass\r\n",
        "    outputs = net(inputs)\r\n",
        "\r\n",
        "    # Apply the loss\r\n",
        "    loss = cost_function(outputs,targets)\r\n",
        "      \r\n",
        "    # Backward pass\r\n",
        "    loss.backward()\r\n",
        "    \r\n",
        "    # Update parameters\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    # Reset the optimizer\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # Better print something, no?\r\n",
        "    samples+=inputs.shape[0]\r\n",
        "    cumulative_loss += loss.item()\r\n",
        "    _, predicted = outputs.max(1)\r\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_data(batch_size, test_batch_size=256):\r\n",
        "  \r\n",
        "  # Prepare data transformations and then combine them sequentially\r\n",
        "  transform = list()\r\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\r\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\r\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\r\n",
        "\r\n",
        "  # Load data\r\n",
        "  full_training_data = torchvision.datasets.MNIST('./dataset', train=True, transform=transform, download=True) \r\n",
        "  test_data = torchvision.datasets.MNIST('./dataset', train=False, transform=transform, download=True) \r\n",
        "  \r\n",
        "\r\n",
        "  # Create train and validation splits\r\n",
        "  num_samples = len(full_training_data)\r\n",
        "  training_samples = int(num_samples*0.5+1)\r\n",
        "  validation_samples = num_samples - training_samples\r\n",
        "\r\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\r\n",
        "  \r\n",
        "  # Initialize dataloaders\r\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=8)       #before num_workers=4\r\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=8) #before num_workers=4\r\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=8) #before num_workers=4\r\n",
        "  \r\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "'''\r\n",
        "Input arguments\r\n",
        "  batch_size: Size of a mini-batch\r\n",
        "  device: GPU where you want to train your network\r\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\r\n",
        "  momentum: Momentum for SGD optimizer\r\n",
        "  epochs: Number of epochs for training the network\r\n",
        "'''\r\n",
        "\r\n",
        "def main(batch_size=1024, \r\n",
        "         device='cuda:0', \r\n",
        "         learning_rate=0.01, \r\n",
        "         weight_decay=0.000001, \r\n",
        "         momentum=0.9, \r\n",
        "         epochs=25):\r\n",
        "  \r\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\r\n",
        "  \r\n",
        "  net =  LeNet().to(device)\r\n",
        "  \r\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\r\n",
        "  \r\n",
        "  cost_function = get_cost_function()\r\n",
        "\r\n",
        "  print('Before training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')\r\n",
        "\r\n",
        "  for e in range(epochs):\r\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\r\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "    print('Epoch: {:d}'.format(e+1))\r\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "    print('-----------------------------------------------------')\r\n",
        "  print('After training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Free GPU memory\r\n",
        "torch.cuda.empty_cache()\r\n",
        "import gc\r\n",
        "gc.collect()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "main()"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}