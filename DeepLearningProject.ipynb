{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4h0F8jKyF6wPwdT4Qef6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb](https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Project"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "# import necessary libraries\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import pandas as pd\r\n",
        "from skimage import io, transform\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision import transforms, utils\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import os\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "\r\n",
        "# print cuda info\r\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\r\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")\r\n",
        "print(f\"Cuda device used: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available: True\n",
            "Cuda device count: 1\n",
            "Cuda device used: GeForce RTX 2060 SUPER\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing\r\n",
        "Make sure to extract the zip into the 'dataset' folder"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [
        "annotations_frame = pd.read_csv('dataset/annotations_train.csv')\r\n",
        "print(annotations_frame.iloc[0, 0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "474\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "source": [
        "def setupLabelsDict(annotations_frame):\r\n",
        "   \r\n",
        "    labels = {}\r\n",
        "    index = 0\r\n",
        "    for i in list(annotations_frame):\r\n",
        "        if(i != \"id\"):\r\n",
        "            for j in range(min(annotations_frame[i]), max(annotations_frame[i])+1):\r\n",
        "                labels[f\"{i}-{j}\"] = index\r\n",
        "                index+=1\r\n",
        "    return labels\r\n",
        "\r\n",
        "def getTargetEncoding(id, annotations_frame, labels):\r\n",
        "    encoding = [0 for _ in range(len(labels))]\r\n",
        "    labels_df = annotations_frame.loc[annotations_frame['id'] == id]\r\n",
        "    for label, content in labels_df.items():\r\n",
        "        if(label != 'id'):\r\n",
        "            encoding[labels[\"%s-%s\" % (label, labels_df[label].iloc[0])]] += 1\r\n",
        "    return encoding"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "source": [
        "class PeopleDataset(Dataset):\r\n",
        "    \"\"\"People with annotations dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, frame_with_labels, root_dir, labels, train, transform=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.annotations_frame = frame_with_labels\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.transform = transform\r\n",
        "        self.img_files = [f for f in listdir(root_dir)]\r\n",
        "        self.labels = labels\r\n",
        "        self.train = train\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.annotations_frame)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        if train:\r\n",
        "            img_name = os.path.join(self.root_dir,self.annotations_frame.iloc[idx, 0])\r\n",
        "            image = io.imread(img_name)\r\n",
        "            encoding = getTargetEncoding(self.annotations_frame.iloc[idx, 0],self.annotations_frame, self.labels)\r\n",
        "            sample = (image, torch.tensor(encoding))\r\n",
        "            return sample\r\n",
        "        else:\r\n",
        "            image = io.imread(self.img_files[idx])\r\n",
        "            sample = image\r\n",
        "            return sample"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "source": [
        "def convertAnnotationsFrame(annotations_frame, train_dir):\r\n",
        "    annotations_frame = pd.read_csv('dataset/annotations_train.csv')\r\n",
        "\r\n",
        "    img_files = [f for f in listdir(train_dir)]\r\n",
        "\r\n",
        "    augmented_annotations_list = [] \r\n",
        "    for entry in annotations_frame.itertuples():\r\n",
        "        for i in img_files:\r\n",
        "            if(int(entry[1]) == int(i.split(\"_\")[0])):\r\n",
        "                img_with_annotation = {\r\n",
        "                    \"id\": i, \r\n",
        "                    \"age\": entry[2], \r\n",
        "                    \"backpack\":entry[3],\r\n",
        "                    \"bag\":entry[4],\r\n",
        "                    \"handbag\":entry[5],\r\n",
        "                    \"clothes\":entry[6],\r\n",
        "                    \"down\":entry[7],\r\n",
        "                    \"up\":entry[8],\r\n",
        "                    \"hair\":entry[9],\r\n",
        "                    \"hat\":entry[10],\r\n",
        "                    \"gender\":entry[11],\r\n",
        "                    \"upblack\":entry[12],\r\n",
        "                    \"upwhite\":entry[13],\r\n",
        "                    \"upred\":entry[14],\r\n",
        "                    \"uppurple\":entry[15],\r\n",
        "                    \"upyellow\":entry[16],\r\n",
        "                    \"upgray\":entry[17],\r\n",
        "                    \"upblue\":entry[18],\r\n",
        "                    \"upgreen\":entry[19],\r\n",
        "                    \"downblack\":entry[20],\r\n",
        "                    \"downwhite\":entry[21],\r\n",
        "                    \"downpink\":entry[22],\r\n",
        "                    \"downpurple\":entry[23],\r\n",
        "                    \"downyellow\":entry[24],\r\n",
        "                    \"downgray\":entry[25],\r\n",
        "                    \"downblue\":entry[26],\r\n",
        "                    \"downgreen\":entry[27],\r\n",
        "                    \"downbrown\":entry[28]\r\n",
        "                }\r\n",
        "                augmented_annotations_list.append(img_with_annotation)\r\n",
        "\r\n",
        "    augmented_annotations_frame = pd.DataFrame(augmented_annotations_list)\r\n",
        "    return augmented_annotations_frame\r\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "#labels = setupLabelsDict(augmented_annotations_frame)\r\n",
        "#print(augmented_annotations_frame.head())\r\n",
        "#id = augmented_annotations_frame.iloc[1, 0]\r\n",
        "#print(id)\r\n",
        "#labels_df = augmented_annotations_frame.loc[augmented_annotations_frame['id'] == id]\r\n",
        "#print(labels_df)\r\n",
        "#encoding = getTargetEncoding(augmented_annotations_frame.iloc[1, 0],augmented_annotations_frame, labels)\r\n",
        "#print(encoding)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "#labels = setupLabelsDict(annotations_frame)\r\n",
        "#print(len(labels))\r\n",
        "#print(labels)\r\n",
        "#people_dataset = PeopleDataset(frame_with_labels=augmented_annotations_frame,\r\n",
        "#                                    root_dir='./dataset/train',\r\n",
        "#                                    labels=labels)\r\n",
        "#\r\n",
        "#print(\"Dataset Initialized\")\r\n",
        "#dataloader = DataLoader(people_dataset, batch_size=1,\r\n",
        "#                        shuffle=True, num_workers=0)\r\n",
        "#print(\"DataLoader Initialized\")\r\n",
        "#print(len(people_dataset))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "#for batch_idx, (inputs, targets) in enumerate(dataloader):\r\n",
        "#    print(batch_idx, type(inputs), targets)\r\n",
        "#    if batch_idx == 3:\r\n",
        "#        break"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network\r\n",
        "## Fine tuning AlexNet"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "source": [
        "'''\r\n",
        "Input arguments\r\n",
        "  num_classes: number of classes in the dataset.\r\n",
        "               This is equal to the number of output neurons.\r\n",
        "'''\r\n",
        "\r\n",
        "def initialize_alexnet(num_classes):\r\n",
        "  # load the pre-trained Alexnet\r\n",
        "  alexnet = torchvision.models.alexnet(pretrained=True)\r\n",
        "  \r\n",
        "  # get the number of neurons in the penultimate layer\r\n",
        "  in_features = alexnet.classifier[6].in_features\r\n",
        "  \r\n",
        "  # re-initalize the output layer\r\n",
        "  alexnet.classifier[6] = torch.nn.Linear(in_features=in_features, \r\n",
        "                                          out_features=num_classes)\r\n",
        "  \r\n",
        "  return alexnet"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "source": [
        "def get_cost_function():\r\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\r\n",
        "  return cost_function"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "source": [
        "def get_optimizer(net, lr):\r\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))\r\n",
        "  return optimizer"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  with torch.no_grad():\r\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\r\n",
        "      # Load data into GPU\r\n",
        "      inputs = inputs.to(device)\r\n",
        "      targets = targets.to(device)\r\n",
        "        \r\n",
        "      # Forward pass\r\n",
        "      outputs = net(inputs)\r\n",
        "\r\n",
        "      # Apply the loss\r\n",
        "      loss = cost_function(outputs, targets)\r\n",
        "\r\n",
        "      # Better print something\r\n",
        "      samples+=inputs.shape[0]\r\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\r\n",
        "      _, predicted = outputs.max(1)\r\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\r\n",
        "\r\n",
        "\r\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  \r\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\r\n",
        "    # Load data into GPU\r\n",
        "    inputs = inputs.to(device)\r\n",
        "    targets = targets.to(device)\r\n",
        "      \r\n",
        "    # Forward pass\r\n",
        "    outputs = net(inputs)\r\n",
        "\r\n",
        "    # Apply the loss\r\n",
        "    loss = cost_function(outputs,targets)\r\n",
        "      \r\n",
        "    # Backward pass\r\n",
        "    loss.backward()\r\n",
        "    \r\n",
        "    # Update parameters\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    # Reset the optimizer\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # Better print something, no?\r\n",
        "    samples+=inputs.shape[0]\r\n",
        "    cumulative_loss += loss.item()\r\n",
        "    _, predicted = outputs.max(1)\r\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "def get_data(augmented_annotations_frame, labels, batch_size, img_root, test_batch_size=256):\r\n",
        "  \r\n",
        "  # Prepare data transformations and then combine them sequentially\r\n",
        "  # transform = list()\r\n",
        "  # transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\r\n",
        "  # transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\r\n",
        "  # transform = T.Compose(transform)                          # Composes the above transformations into one.\r\n",
        "\r\n",
        "  # Load data\r\n",
        "  full_training_data = PeopleDataset(frame_with_labels=augmented_annotations_frame,\r\n",
        "                                      root_dir=\"%s/train\" % (img_root),\r\n",
        "                                      labels=labels,\r\n",
        "                                      train=True)\r\n",
        "  test_data = PeopleDataset(frame_with_labels=augmented_annotations_frame,\r\n",
        "                                      root_dir=\"%s/test\" % (img_root),\r\n",
        "                                      labels=labels,\r\n",
        "                                      train=False)\r\n",
        "\r\n",
        "  #print(\"Dataset Initialized\")\r\n",
        "  #dataloader = DataLoader(people_dataset, batch_size=,\r\n",
        "  #                        shuffle=True, num_workers=0)\r\n",
        "  #print(\"DataLoader Initialized\")\r\n",
        "  #print(len(people_dataset))\r\n",
        "  #full_training_data = torchvision.datasets.MNIST('./dataset', train=True, transform=transform, download=True) \r\n",
        "  #test_data = torchvision.datasets.MNIST('./dataset', train=False, transform=transform, download=True) \r\n",
        "  \r\n",
        "\r\n",
        "  # Create train and validation splits\r\n",
        "  num_samples = len(full_training_data)\r\n",
        "  training_samples = int(num_samples*0.5+1)\r\n",
        "  validation_samples = num_samples - training_samples\r\n",
        "\r\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\r\n",
        "  \r\n",
        "  # Initialize dataloaders\r\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=0)       #before num_workers=4\r\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=0) #before num_workers=4\r\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=0) #before num_workers=4\r\n",
        "  \r\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "source": [
        "def main(batch_size=128, \r\n",
        "         device='cuda:0', \r\n",
        "         learning_rate=0.001, \r\n",
        "         epochs=50, \r\n",
        "         img_root='./dataset'):\r\n",
        "  \r\n",
        "  writer = SummaryWriter(log_dir=\"runs/exp1\")\r\n",
        "\r\n",
        "  annotations_frame = pd.read_csv(\"./dataset/annotations_train.csv\")\r\n",
        "  augmented_annotations_frame = convertAnnotationsFrame(annotations_frame, \"%s/train\" % (img_root))\r\n",
        "  labels = setupLabelsDict(augmented_annotations_frame)\r\n",
        "\r\n",
        "  # Instantiates dataloaders\r\n",
        "  train_loader, val_loader, test_loader = get_data(augmented_annotations_frame=augmented_annotations_frame, labels=labels, batch_size=batch_size, img_root=img_root)\r\n",
        "  \r\n",
        "  # Instantiates the model\r\n",
        "  net = initialize_alexnet(num_classes=len(labels)).to(device)\r\n",
        "  \r\n",
        "  # Instantiates the optimizer\r\n",
        "  optimizer = get_optimizer(net, learning_rate)\r\n",
        "  \r\n",
        "  # Instantiates the cost function\r\n",
        "  cost_function = get_cost_function()\r\n",
        "\r\n",
        "  print('Before training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')\r\n",
        "\r\n",
        "  for e in range(epochs):\r\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\r\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "    print('Epoch: {:d}'.format(e+1))\r\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "    print('-----------------------------------------------------')\r\n",
        "  print('After training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\r\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "source": [
        "# Free GPU memory\r\n",
        "torch.cuda.empty_cache()\r\n",
        "import gc\r\n",
        "gc.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13649"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "source": [
        "main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 3, 11, 11], expected input[128, 128, 64, 3] to have 3 channels, but got 128 channels instead",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-63-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-61-07133c741fbf>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(batch_size, device, learning_rate, epochs, img_root)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Before training:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m   \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[1;31m#test_loss, test_accuracy = test(net, test_loader, cost_function)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-59-62603e77e2de>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(net, data_loader, cost_function, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m       \u001b[1;31m# Apply the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 11, 11], expected input[128, 128, 64, 3] to have 3 channels, but got 128 channels instead"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}