{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject_withaugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb](https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject.ipynb)"
      ],
      "metadata": {
        "id": "W6jMA8w_8oOH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "id": "CaPQ91Z78oOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba43fb3-02da-4d86-fc0d-d9f959c4288d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip \"/content/drive/MyDrive/UNITN/5Â° anno/Deep Learning 2021/dataset.zip\" -d dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "bSxDvJ0D8oOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Project - People ReID"
      ],
      "metadata": {
        "id": "jg8kbiHv8oOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "# import necessary libraries\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import pandas as pd\r\n",
        "from skimage import io, transform\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision import transforms, utils\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import os\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from PIL import Image\r\n",
        "import random\r\n",
        "random.seed(10)\r\n",
        "# print cuda info\r\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\r\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available: True\n",
            "Cuda device count: 1\n"
          ]
        }
      ],
      "metadata": {
        "id": "NUDHglK08oOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d9196b-2fad-4ee4-f645-563687ce2709"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing\n",
        "Make sure to extract the zip into the 'dataset' folder"
      ],
      "metadata": {
        "id": "O1_3ydYU8oOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "def setupLabelsDict(annotations_frame):\r\n",
        "   \r\n",
        "    labels = {}\r\n",
        "    index = 0\r\n",
        "    for i in list(annotations_frame):\r\n",
        "        if(i != \"id\"):\r\n",
        "            for j in range(min(annotations_frame[i]), max(annotations_frame[i])+1):\r\n",
        "                labels[f\"{i}-{j}\"] = index\r\n",
        "                index+=1\r\n",
        "    return labels\r\n",
        "\r\n",
        "def getTargetEncoding(id, annotations_frame, labels):\r\n",
        "    encoding = [0 for _ in range(len(labels))]\r\n",
        "    labels_df = annotations_frame.loc[annotations_frame['id'] == id]\r\n",
        "    for label, content in labels_df.items():\r\n",
        "        if(label != 'id'):\r\n",
        "            encoding[labels[\"%s-%s\" % (label, labels_df[label].iloc[0])]] += 1\r\n",
        "    return encoding"
      ],
      "outputs": [],
      "metadata": {
        "id": "NaU_EAEt8oOP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "def convertAnnotationsFrame(annotations_frame, train_dir):\r\n",
        "    annotations_frame = pd.read_csv('dataset/annotations_train.csv')\r\n",
        "\r\n",
        "    img_files = [f for f in listdir(train_dir)]\r\n",
        "\r\n",
        "    augmented_annotations_list = [] \r\n",
        "    for entry in annotations_frame.itertuples():\r\n",
        "        for i in img_files:\r\n",
        "            if(int(entry[1]) == int(i.split(\"_\")[0])):\r\n",
        "                img_with_annotation = {\r\n",
        "                    \"id\": i, \r\n",
        "                    \"age\": entry[2], \r\n",
        "                    \"backpack\":entry[3],\r\n",
        "                    \"bag\":entry[4],\r\n",
        "                    \"handbag\":entry[5],\r\n",
        "                    \"clothes\":entry[6],\r\n",
        "                    \"down\":entry[7],\r\n",
        "                    \"up\":entry[8],\r\n",
        "                    \"hair\":entry[9],\r\n",
        "                    \"hat\":entry[10],\r\n",
        "                    \"gender\":entry[11],\r\n",
        "                    \"upblack\":entry[12],\r\n",
        "                    \"upwhite\":entry[13],\r\n",
        "                    \"upred\":entry[14],\r\n",
        "                    \"uppurple\":entry[15],\r\n",
        "                    \"upyellow\":entry[16],\r\n",
        "                    \"upgray\":entry[17],\r\n",
        "                    \"upblue\":entry[18],\r\n",
        "                    \"upgreen\":entry[19],\r\n",
        "                    \"downblack\":entry[20],\r\n",
        "                    \"downwhite\":entry[21],\r\n",
        "                    \"downpink\":entry[22],\r\n",
        "                    \"downpurple\":entry[23],\r\n",
        "                    \"downyellow\":entry[24],\r\n",
        "                    \"downgray\":entry[25],\r\n",
        "                    \"downblue\":entry[26],\r\n",
        "                    \"downgreen\":entry[27],\r\n",
        "                    \"downbrown\":entry[28]\r\n",
        "                }\r\n",
        "                augmented_annotations_list.append(img_with_annotation)\r\n",
        "\r\n",
        "    augmented_annotations_frame = pd.DataFrame(augmented_annotations_list)\r\n",
        "    return augmented_annotations_frame\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "b2ZU-pnv8oOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "#labels = setupLabelsDict(augmented_annotations_frame)\r\n",
        "#print(augmented_annotations_frame.head())\r\n",
        "#id = augmented_annotations_frame.iloc[1, 0]\r\n",
        "#print(id)\r\n",
        "#labels_df = augmented_annotations_frame.loc[augmented_annotations_frame['id'] == id]\r\n",
        "#print(labels_df)\r\n",
        "#encoding = getTargetEncoding(augmented_annotations_frame.iloc[1, 0],augmented_annotations_frame, labels)\r\n",
        "#print(encoding)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Mj5sCxu18oOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "#labels = setupLabelsDict(annotations_frame)\r\n",
        "#print(len(labels))\r\n",
        "#print(labels)\r\n",
        "#people_dataset = PeopleDataset(frame_with_labels=augmented_annotations_frame,\r\n",
        "#                                    root_dir='./dataset/train',\r\n",
        "#                                    labels=labels)\r\n",
        "#\r\n",
        "#print(\"Dataset Initialized\")\r\n",
        "#dataloader = DataLoader(people_dataset, batch_size=1,\r\n",
        "#                        shuffle=True, num_workers=0)\r\n",
        "#print(\"DataLoader Initialized\")\r\n",
        "#print(len(people_dataset))"
      ],
      "outputs": [],
      "metadata": {
        "id": "t-O1rCrz8oOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "#for batch_idx, (inputs, targets) in enumerate(dataloader):\r\n",
        "#    print(batch_idx, type(inputs), targets)\r\n",
        "#    if batch_idx == 3:\r\n",
        "#        break"
      ],
      "outputs": [],
      "metadata": {
        "id": "s891cbaj8oOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network\r\n",
        "## Siamese Network"
      ],
      "metadata": {
        "id": "WwZMA17l8oOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "'''\r\n",
        "Input arguments\r\n",
        "  num_classes: number of classes in the dataset.\r\n",
        "               This is equal to the number of output neurons.\r\n",
        "'''\r\n",
        "\r\n",
        "class Siamese(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, resnet):\r\n",
        "        super(Siamese, self).__init__()\r\n",
        "        self.resnet = resnet\r\n",
        "        self.resnet.fc = nn.Sequential(nn.Linear(2048, 1024), nn.Sigmoid())\r\n",
        "        self.out = nn.Linear(1024, 1)\r\n",
        "\r\n",
        "    def forward_one(self, x):\r\n",
        "        x = self.resnet(x)\r\n",
        "        #x = x.view(x.size()[0], -1)\r\n",
        "        #x = self.liner(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def forward(self, x1, x2):\r\n",
        "        out1 = self.forward_one(x1)\r\n",
        "        out2 = self.forward_one(x2)\r\n",
        "        dis = torch.abs(out1 - out2)\r\n",
        "        out = self.out(dis)\r\n",
        "        #  return self.sigmoid(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "def initialize_alexnet(num_classes):\r\n",
        "  # load the pre-trained Alexnet\r\n",
        "  #alexnet = torchvision.models.alexnet(pretrained=True)\r\n",
        "  resnet = torchvision.models.resnet50(pretrained=True, progress=False)\r\n",
        "  num_features = resnet.fc.in_features\r\n",
        "  resnet.fc = torch.nn.Sequential(\r\n",
        "    torch.nn.Linear(in_features=num_features, out_features=num_classes),\r\n",
        "    torch.nn.Sigmoid()\r\n",
        "  )\r\n",
        "  #print(resnet)\r\n",
        "\r\n",
        "  # get the number of neurons in the penultimate layer\r\n",
        "  #in_features = alexnet.classifier[6].in_features\r\n",
        "  \r\n",
        "  # re-initalize the output layer\r\n",
        "  #alexnet.classifier[6] = torch.nn.Sequential(\r\n",
        "  #  torch.nn.Linear(in_features=in_features, out_features=num_classes),\r\n",
        "  #  torch.nn.Sigmoid()\r\n",
        "  #)\r\n",
        "  return resnet"
      ],
      "outputs": [],
      "metadata": {
        "id": "9__1oD_z8oOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function"
      ],
      "metadata": {
        "id": "cy4p3NWR8oOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "def get_cost_function():\r\n",
        "  cost_function = torch.nn.BCEWithLogitsLoss(size_average=True)\r\n",
        "  return cost_function"
      ],
      "outputs": [],
      "metadata": {
        "id": "C6rgB5208oOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer"
      ],
      "metadata": {
        "id": "uCxk8k3T8oOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "def get_optimizer(net, lr):\r\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))\r\n",
        "  return optimizer"
      ],
      "outputs": [],
      "metadata": {
        "id": "FmyQy5dx8oOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "def test(net, data_loader, cost_function, num_classes, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  with torch.no_grad():\r\n",
        "    for batch_idx, (img1, img2, targets) in enumerate(data_loader):\r\n",
        "      # Load data into GPU\r\n",
        "      img1 = img1.to(device)\r\n",
        "      img2 = img2.to(device)\r\n",
        "      targets = targets.to(torch.float32) #converting to float for BCELoss\r\n",
        "      targets = targets.to(device)\r\n",
        "      #print(inputs.size())\r\n",
        "      #print(input)\r\n",
        "      #print(targets.size())\r\n",
        "      #print(targets)\r\n",
        "        \r\n",
        "      # Forward pass\r\n",
        "      outputs = net.forward(img1, img2)\r\n",
        "      #print(outputs)\r\n",
        "      # Apply the loss\r\n",
        "      outputs = torch.squeeze(outputs,1)\r\n",
        "      #print(outputs.size(), targets.size())\r\n",
        "      loss = cost_function(outputs, targets)\r\n",
        "\r\n",
        "      # Better print something\r\n",
        "      samples+=img1.shape[0]\r\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\r\n",
        "      predicted = torch.round(outputs)\r\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()/num_classes\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\r\n",
        "\r\n",
        "\r\n",
        "def train(net,data_loader,optimizer,cost_function, num_classes, device='cuda:0'):\r\n",
        "  samples = 0.\r\n",
        "  cumulative_loss = 0.\r\n",
        "  cumulative_accuracy = 0.\r\n",
        "\r\n",
        "  \r\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  for batch_idx, (img1, img2, targets) in enumerate(data_loader):\r\n",
        "    # Load data into GPU\r\n",
        "    img1 = img1.to(device)\r\n",
        "    img2 = img2.to(device)\r\n",
        "    targets = targets.to(torch.float32) #converting to float for BCELoss\r\n",
        "    targets = targets.to(device)\r\n",
        "\r\n",
        "    # Forward pass\r\n",
        "    outputs = net.forward(img1, img2)\r\n",
        "\r\n",
        "    # Apply the loss\r\n",
        "    outputs = torch.squeeze(outputs,1)\r\n",
        "    #print(outputs.size(), targets.size())\r\n",
        "    loss = cost_function(outputs,targets)\r\n",
        "      \r\n",
        "    # Backward pass\r\n",
        "    loss.backward()\r\n",
        "    \r\n",
        "    # Update parameters\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    # Reset the optimizer\r\n",
        "    optimizer.zero_grad()\r\n",
        "\r\n",
        "    # Better print something, no?\r\n",
        "    samples+=img1.shape[0]\r\n",
        "    cumulative_loss += loss.item()\r\n",
        "    predicted = torch.round(outputs)\r\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()/num_classes\r\n",
        "\r\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "outputs": [],
      "metadata": {
        "id": "IE7Jo7Z08oOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "class PeopleTrainDataset(Dataset):\r\n",
        "    \"\"\"People with annotations dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, X1, X2, Y, root_dir, transform):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.transform = transform\r\n",
        "        self.X1 = X1\r\n",
        "        self.X2 = X2\r\n",
        "        self.Y = Y\r\n",
        "        self.root_dir = root_dir\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.Y)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        img_name1 = self.X1[idx]\r\n",
        "        img_name2 = self.X2[idx]\r\n",
        "\r\n",
        "        image1 = Image.open(\"%s/%s\" % (self.root_dir, img_name1))\r\n",
        "        image2 = Image.open(\"%s/%s\" % (self.root_dir, img_name2))\r\n",
        "        if self.transform != None:\r\n",
        "          image1 = self.transform(image1)\r\n",
        "          image2 = self.transform(image2)\r\n",
        "        else:\r\n",
        "          image1 = T.ToTensor()(image1)\r\n",
        "          image2 = T.ToTensor()(image2)\r\n",
        "        image1 = F.interpolate(image1, size=128)  \r\n",
        "        image2 = F.interpolate(image2, size=128)  \r\n",
        "\r\n",
        "        sample = (image1, image2, self.Y[idx])\r\n",
        "        return sample\r\n",
        "\r\n",
        "class PeopleTestDataset(Dataset):\r\n",
        "    \"\"\"People with annotations dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, root_dir):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            csv_file (string): Path to the csv file with annotations.\r\n",
        "            root_dir (string): Directory with all the images.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.root_dir = root_dir\r\n",
        "        self.img_files = [f for f in listdir(root_dir)]\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.annotations_frame)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "        \r\n",
        "        image = io.imread(self.img_files[idx])\r\n",
        "        image = T.ToTensor()(image)\r\n",
        "        image = F.interpolate(image, size=128)  \r\n",
        "        return image"
      ],
      "outputs": [],
      "metadata": {
        "id": "fINP--L08oOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "def split_training_data(root_dir):\r\n",
        "  full_dataset_X1 = []\r\n",
        "  full_dataset_X2 = []\r\n",
        "  full_dataset_Y = []\r\n",
        "  img_files = [f for f in listdir(root_dir)]\r\n",
        "  random.shuffle(img_files)\r\n",
        "  if(len(img_files) % 2 != 0):\r\n",
        "    img_files = img_files[:-1]\r\n",
        "\r\n",
        "  for idx, img in enumerate(img_files):\r\n",
        "    if(idx % 2 == 0):\r\n",
        "      full_dataset_X1.append(img)\r\n",
        "    if(idx % 2 != 0):\r\n",
        "      full_dataset_X2.append(img)\r\n",
        "      if (int(full_dataset_X1[-1].split(\"_\")[0]) == int(full_dataset_X2[-1].split(\"_\")[0])):\r\n",
        "        full_dataset_Y.append(torch.tensor(1))\r\n",
        "      else:\r\n",
        "        full_dataset_Y.append(torch.tensor(0))\r\n",
        "\r\n",
        "  print(len(full_dataset_X1), len(full_dataset_X2), len(full_dataset_Y))\r\n",
        "  val_X1 = []\r\n",
        "  val_X2 = []\r\n",
        "  val_Y = []\r\n",
        "  train_X1 = []\r\n",
        "  train_X2 = []\r\n",
        "  train_Y = []\r\n",
        "  for idx, _ in enumerate(full_dataset_Y):\r\n",
        "    if (idx <= len(full_dataset_Y)*0.2):\r\n",
        "      val_X1.append(full_dataset_X1[idx])\r\n",
        "      val_X2.append(full_dataset_X2[idx])\r\n",
        "      val_Y.append(full_dataset_Y[idx])\r\n",
        "    else:\r\n",
        "      train_X1.append(full_dataset_X1[idx])\r\n",
        "      train_X2.append(full_dataset_X2[idx])\r\n",
        "      train_Y.append(full_dataset_Y[idx])\r\n",
        "\r\n",
        "  return train_X1, train_X2, train_Y, val_X1, val_X2, val_Y\r\n",
        "\r\n",
        "def get_data(batch_size, img_root, test_batch_size=256):\r\n",
        "  \r\n",
        "  # Prepare data transformations and then combine them sequentially\r\n",
        "  # transform = list()\r\n",
        "  # transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\r\n",
        "  # transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\r\n",
        "  # transform = T.Compose(transform)                          # Composes the above transformations into one.\r\n",
        "  torchvision_transform = T.Compose([\r\n",
        "      T.Resize((256, 256)), \r\n",
        "      T.RandomCrop(224),\r\n",
        "      T.RandomHorizontalFlip(),\r\n",
        "      T.ToTensor(),\r\n",
        "      T.Normalize(\r\n",
        "          mean=[0.485, 0.456, 0.406],\r\n",
        "          std=[0.229, 0.224, 0.225],\r\n",
        "      )\r\n",
        "  ])\r\n",
        "\r\n",
        "  # Get splitted data\r\n",
        "  train_X1, train_X2, train_Y, val_X1, val_X2, val_Y = split_training_data(root_dir=\"%s/train\" % (img_root))\r\n",
        "\r\n",
        "  # Load data\r\n",
        "  training_data = PeopleTrainDataset(X1=train_X1,\r\n",
        "                                     X2=train_X2,\r\n",
        "                                     Y=train_Y,\r\n",
        "                                     root_dir=\"%s/train\" % (img_root),\r\n",
        "                                     transform=None)\r\n",
        "  \r\n",
        "  validation_data = PeopleTrainDataset(X1=val_X1,\r\n",
        "                                     X2=val_X2,\r\n",
        "                                     Y=val_Y,\r\n",
        "                                     root_dir=\"%s/train\" % (img_root),\r\n",
        "                                     transform=None)\r\n",
        "  \r\n",
        "  #test_data = PeopleTestDataset(root_dir=\"%s/test\" % (img_root))\r\n",
        "\r\n",
        "  #print(\"Dataset Initialized\")\r\n",
        "  #dataloader = DataLoader(people_dataset, batch_size=,\r\n",
        "  #                        shuffle=True, num_workers=0)\r\n",
        "  #print(\"DataLoader Initialized\")\r\n",
        "  #print(len(people_dataset))\r\n",
        "  #full_training_data = torchvision.datasets.MNIST('./dataset', train=True, transform=transform, download=True) \r\n",
        "  #test_data = torchvision.datasets.MNIST('./dataset', train=False, transform=transform, download=True) \r\n",
        "  \r\n",
        "\r\n",
        "  # Create train and validation splits\r\n",
        "  #num_samples = len(full_training_data)\r\n",
        "  #training_samples = int(num_samples*0.5+1)\r\n",
        "  #validation_samples = num_samples - training_samples\r\n",
        "#\r\n",
        "  #training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\r\n",
        "  \r\n",
        "  # Initialize dataloaders\r\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=0)       #before num_workers=4\r\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=0) #before num_workers=4\r\n",
        "  #test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=0) #before num_workers=4\r\n",
        "  test_loader = None\r\n",
        "  \r\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "outputs": [],
      "metadata": {
        "id": "PVD-FE2e8oOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "def log_values(writer, step, loss, accuracy, prefix):\r\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\r\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\r\n",
        "\r\n",
        "def main(batch_size=128, \r\n",
        "         device='cuda:0', \r\n",
        "         learning_rate=0.001, \r\n",
        "         epochs=5, \r\n",
        "         img_root='./dataset'):\r\n",
        "  from torch.utils.tensorboard import SummaryWriter\r\n",
        "  writer = SummaryWriter(log_dir=\"runs/exp2\")\r\n",
        "\r\n",
        "  annotations_frame = pd.read_csv(\"./dataset/annotations_train.csv\")\r\n",
        "  augmented_annotations_frame = convertAnnotationsFrame(annotations_frame, \"%s/train\" % (img_root))\r\n",
        "  labels = setupLabelsDict(augmented_annotations_frame)\r\n",
        "\r\n",
        "  # Instantiates dataloaders\r\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size=batch_size, img_root=img_root)\r\n",
        "  \r\n",
        "  # Instantiates the model\r\n",
        "  net = initialize_alexnet(num_classes=len(labels)).to(device)\r\n",
        "  net.load_state_dict(torch.load(\"./models/resnet50_20epoch.pth\"))\r\n",
        "  net = Siamese(net)\r\n",
        "  net.to(device)\r\n",
        "    \r\n",
        "  # Instantiates the optimizer\r\n",
        "  optimizer = get_optimizer(net, learning_rate)\r\n",
        "  \r\n",
        "  # Instantiates the cost function\r\n",
        "  cost_function = get_cost_function()\r\n",
        "\r\n",
        "  print('Before training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function, num_classes=len(labels))\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function, num_classes=len(labels))\r\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "\r\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"Train\")\r\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\r\n",
        "\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')\r\n",
        "\r\n",
        "  for e in range(epochs):\r\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function, num_classes=len(labels))\r\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function, num_classes=len(labels))\r\n",
        "    print('Epoch: {:d}'.format(e+1))\r\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "    print('-----------------------------------------------------')\r\n",
        "    log_values(writer, e, train_loss, train_accuracy, \"Train\")\r\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\r\n",
        "  print('After training:')\r\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function, num_classes=len(labels))\r\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function, num_classes=len(labels))\r\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\r\n",
        "  log_values(writer, e, train_loss, train_accuracy, \"Train\")\r\n",
        "  log_values(writer, e, val_loss, val_accuracy, \"Validation\")\r\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\r\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\r\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\r\n",
        "  print('-----------------------------------------------------')\r\n",
        "  # Closes the logger\r\n",
        "  writer.close()\r\n",
        "  return net"
      ],
      "outputs": [],
      "metadata": {
        "id": "QnndkhgI8oOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "# Free GPU memory\r\n",
        "torch.cuda.empty_cache()\r\n",
        "import gc\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "# clear runs\r\n",
        "#! rm -r runs\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir=runs/exp2/"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-e886325fe7b85192\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-e886325fe7b85192\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "id": "sqom_4Hz8oOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18945a27-a3b1-46f2-b275-d30598ad8370"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "net = main()\r\n",
        "torch.save(net.state_dict(), \"./resnet_with_augmentation_20epoch\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6494 6494 6494\n",
            "Before training:\n",
            "\t Training loss 0.00515, Training accuracy 1.78\n",
            "\t Validation loss 0.00301, Validation accuracy 1.78\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 5.65 GiB already allocated; 14.32 MiB free; 5.87 GiB reserved in total by PyTorch)",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-54-29c14fc737ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./resnet_with_augmentation_20epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-39-779ec89226d5>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(batch_size, device, learning_rate, epochs, img_root)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: {:d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-53-8cfde639dc2a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, data_loader, optimizer, cost_function, num_classes, device)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# Apply the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-29-26a60fafe692>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mout2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mdis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-29-26a60fafe692>\u001b[0m in \u001b[0;36mforward_one\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#x = x.view(x.size()[0], -1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#x = self.liner(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2279\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2281\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2282\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2283\u001b[0m     )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 5.65 GiB already allocated; 14.32 MiB free; 5.87 GiB reserved in total by PyTorch)"
          ]
        }
      ],
      "metadata": {
        "id": "ppQx1-td8oOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28af290-8400-49ba-e9af-daed0b2f5be5"
      }
    }
  ]
}