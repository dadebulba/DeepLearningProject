{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject_withaugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6jMA8w_8oOH"
      },
      "source": [
        "# Deep Learning Project - Person classification task learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v53clazFBVPl"
      },
      "source": [
        "Importing from Google Drive the dataset.zip and extract into dataset folder, change the path with your dataset location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaPQ91Z78oOI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip \"/content/drive/MyDrive/UNITN/5Â° anno/Deep Learning 2021/dataset.zip\" -d dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg8kbiHv8oOL"
      },
      "source": [
        "importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUDHglK08oOM"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "# print cuda info\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1_3ydYU8oOO"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaU_EAEt8oOP"
      },
      "source": [
        "'''\n",
        "Returns: dict where keys are the labels associated with their encoding and the values are the corresponding indexes in the one-hot encoding\n",
        "Input arguments\n",
        "  annotations_frame: dataframe containing annotations associated to people ids\n",
        "'''\n",
        "def setupLabelsDict(annotations_frame):\n",
        "    labels = {}\n",
        "    index = 0\n",
        "    for i in list(annotations_frame):\n",
        "        # id is not part of labels\n",
        "        if(i != \"id\"):\n",
        "            # to build the one-hot encoding I need to know the min and max value range for each label\n",
        "            for j in range(min(annotations_frame[i]), max(annotations_frame[i])+1):\n",
        "                labels[f\"{i}-{j}\"] = index\n",
        "                index+=1\n",
        "    return labels\n",
        "\n",
        "'''\n",
        "Returns: target encoding for corresponding id\n",
        "Input arguments\n",
        "  id: which id has to get the encoding\n",
        "  annotations_frame: used to localize the original target values\n",
        "  labels: newly computed labels\n",
        "'''\n",
        "def getTargetEncoding(id, annotations_frame, labels):\n",
        "    encoding = [0 for _ in range(len(labels))]\n",
        "    labels_df = annotations_frame.loc[annotations_frame['id'] == id]\n",
        "    for label, content in labels_df.items():\n",
        "        if(label != 'id'):\n",
        "            encoding[labels[\"%s-%s\" % (label, labels_df[label].iloc[0])]] += 1\n",
        "    return encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ZU-pnv8oOS"
      },
      "source": [
        "'''\n",
        "Returns: dataframe with annotations for every image of the specified dataset where id is the image name\n",
        "Input arguments\n",
        "  annotations_frame: original dataframe parsed from annotations_train.csv\n",
        "  train_dir: directory containing the images used for training\n",
        "'''\n",
        "def convertAnnotationsFrame(annotations_frame, train_dir):\n",
        "    annotations_frame = pd.read_csv('dataset/annotations_train.csv')\n",
        "\n",
        "    img_files = [f for f in listdir(train_dir)]\n",
        "\n",
        "    augmented_annotations_list = [] \n",
        "    for entry in annotations_frame.itertuples():\n",
        "        for i in img_files:\n",
        "            if(int(entry[1]) == int(i.split(\"_\")[0])):\n",
        "                img_with_annotation = {\n",
        "                    \"id\": i, \n",
        "                    \"age\": entry[2], \n",
        "                    \"backpack\":entry[3],\n",
        "                    \"bag\":entry[4],\n",
        "                    \"handbag\":entry[5],\n",
        "                    \"clothes\":entry[6],\n",
        "                    \"down\":entry[7],\n",
        "                    \"up\":entry[8],\n",
        "                    \"hair\":entry[9],\n",
        "                    \"hat\":entry[10],\n",
        "                    \"gender\":entry[11],\n",
        "                    \"upblack\":entry[12],\n",
        "                    \"upwhite\":entry[13],\n",
        "                    \"upred\":entry[14],\n",
        "                    \"uppurple\":entry[15],\n",
        "                    \"upyellow\":entry[16],\n",
        "                    \"upgray\":entry[17],\n",
        "                    \"upblue\":entry[18],\n",
        "                    \"upgreen\":entry[19],\n",
        "                    \"downblack\":entry[20],\n",
        "                    \"downwhite\":entry[21],\n",
        "                    \"downpink\":entry[22],\n",
        "                    \"downpurple\":entry[23],\n",
        "                    \"downyellow\":entry[24],\n",
        "                    \"downgray\":entry[25],\n",
        "                    \"downblue\":entry[26],\n",
        "                    \"downgreen\":entry[27],\n",
        "                    \"downbrown\":entry[28]\n",
        "                }\n",
        "                augmented_annotations_list.append(img_with_annotation)\n",
        "\n",
        "    augmented_annotations_frame = pd.DataFrame(augmented_annotations_list)\n",
        "    return augmented_annotations_frame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwZMA17l8oOX"
      },
      "source": [
        "## Fine tuning Resnet-50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9__1oD_z8oOX"
      },
      "source": [
        "'''\n",
        "Returns: fine tuned ResNet-50\n",
        "Input arguments\n",
        "  num_classes: number of classes in the dataset.\n",
        "               This is equal to the number of output neurons.\n",
        "'''\n",
        "def initialize_resnet(num_classes):\n",
        "  resnet = torchvision.models.resnet50(pretrained=True)\n",
        "  num_features = resnet.fc.in_features\n",
        "  resnet.fc = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=num_features, out_features=1024),\n",
        "    torch.nn.Linear(in_features=1024, out_features=512),\n",
        "    torch.nn.Linear(in_features=512, out_features=num_classes),\n",
        "    torch.nn.Sigmoid()\n",
        "  )\n",
        "  return resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy4p3NWR8oOY"
      },
      "source": [
        "Cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6rgB5208oOZ"
      },
      "source": [
        "\"\"\"\n",
        "Returns: Binary Cross Entropy Loss function: https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
        "\"\"\"\n",
        "def get_cost_function():\n",
        "  cost_function = torch.nn.BCELoss()\n",
        "  return cost_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCxk8k3T8oOZ"
      },
      "source": [
        "Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmyQy5dx8oOZ"
      },
      "source": [
        "\"\"\"\n",
        "Returns: the Adam optimizer\n",
        "Input arguments\n",
        "  net: network used to setup the optimizer\n",
        "  lr: used learning rate\n",
        "\"\"\"\n",
        "def get_optimizer(net, lr):\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "  return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE7Jo7Z08oOa"
      },
      "source": [
        "\"\"\"\n",
        "Returns: mean loss and accuracy based on passed data and model during test phase\n",
        "Input arguments\n",
        "  net: trained network\n",
        "  data_loader: DataLoader containing test data\n",
        "  cost_function: cost function used to compute the loss\n",
        "  num_classes: number of target classes\n",
        "  device: which device to use during testing phase (default is GPU)\n",
        "\"\"\"\n",
        "def test(net, data_loader, cost_function, num_classes, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(torch.float32) #converting to float for BCELoss\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Compute comulative accuracy and loss\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      predicted = torch.round(outputs)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()/num_classes\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\"\"\"\n",
        "Returns: mean loss and accuracy obtained in current epoch\n",
        "Input arguments\n",
        "  net: network to train\n",
        "  data_loader: DataLoader containing training data\n",
        "  cost_function: cost function used to compute the loss\n",
        "  num_classes: number of target classes\n",
        "  device: which device to use during testing phase (default is GPU)\n",
        "\"\"\"\n",
        "def train(net,data_loader,optimizer,cost_function, num_classes, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(torch.float32) #converting to float for BCELoss\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute comulative accuracy and loss\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    predicted = torch.round(outputs)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()/num_classes\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fINP--L08oOQ"
      },
      "source": [
        "class PeopleTrainDataset(Dataset):\n",
        "    \"\"\"People training dataset containing images and annotations.\"\"\"\n",
        "\n",
        "    def __init__(self, X, Y, transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: list of image names, that are the locations where to load them\n",
        "            Y: list of labels for each image\n",
        "            transform (optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Loading an image\n",
        "        img_name = self.X[idx]\n",
        "        image = Image.open(img_name)\n",
        "\n",
        "        # If transofrm is specified apply to loaded image otherwise simply convert to tensor\n",
        "        if self.transform != None:\n",
        "          image = self.transform(image)\n",
        "        else:\n",
        "          image = T.ToTensor()(image)\n",
        "        \n",
        "        # Bound image to size 128 to input it to network\n",
        "        image = F.interpolate(image, size=128)  \n",
        "        \n",
        "        # Build single sample as tuple of image with corresponding labels\n",
        "        sample = (image, self.Y[idx])\n",
        "        return sample\n",
        "\n",
        "class PeopleTestDataset(Dataset):\n",
        "    \"\"\"People test dataset containing only images.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: Directory with all the images.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.img_files = [f for f in listdir(root_dir)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.annotations_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        # Load image and bound size to 128 to input it to network\n",
        "        image = io.imread(\"%s/%s\" % (self.root_dir, self.img_files[idx]))\n",
        "        image = T.ToTensor()(image)\n",
        "        image = F.interpolate(image, size=128)  \n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVD-FE2e8oOb"
      },
      "source": [
        "\"\"\"\n",
        "Returns: \n",
        "  train_X: inputs used during training\n",
        "  val_X: inputs used during validation\n",
        "  train_Y: targets of training inputs\n",
        "  val_Y: targets of validation inputs\n",
        "Input arguments\n",
        "  root_dir: directory containing the dataset images\n",
        "  annotations_frame: dataframe where each image is associated to a label\n",
        "  labels: newly computed labels\n",
        "\"\"\"\n",
        "def split_training_data(root_dir, annotations_frame, labels):\n",
        "  full_dataset_X = []\n",
        "  full_dataset_Y = []\n",
        "\n",
        "  for idx, img in enumerate(os.listdir(root_dir)):\n",
        "    img_name = os.path.join(root_dir,annotations_frame.iloc[idx, 0])\n",
        "    encoding = getTargetEncoding(annotations_frame.iloc[idx, 0],annotations_frame, labels)\n",
        "    full_dataset_X.append(img_name)\n",
        "    full_dataset_Y.append(torch.tensor(encoding))\n",
        "  ids = annotations_frame['id']\n",
        "  train_id = set()\n",
        "  for id in ids:\n",
        "    train_id.add(int(id.split(\"_\")[0]))\n",
        "  train_id = list(train_id)\n",
        "  random.shuffle(train_id)\n",
        "  val_id = []\n",
        "  for i in range(int(len(train_id)*0.2)):\n",
        "    val_id.append(train_id.pop(i))\n",
        "  \n",
        "  val_X = []\n",
        "  val_Y = []\n",
        "  ids_to_remove = []\n",
        "  for i in val_id:\n",
        "    for idx, img in enumerate(os.listdir(root_dir)):\n",
        "      if int(img.split(\"_\")[0]) == i:\n",
        "        val_X.append(os.path.join(root_dir,img))\n",
        "        full_dataset_X.remove(os.path.join(root_dir,img))\n",
        "        val_Y.append(full_dataset_Y[idx])\n",
        "        ids_to_remove.append(idx)\n",
        "\n",
        "  full_dataset_Y = [value for idx, value in enumerate(full_dataset_Y) if idx not in ids_to_remove] \n",
        "  train_X = full_dataset_X\n",
        "  train_Y = full_dataset_Y\n",
        "\n",
        "  return train_X, val_X, train_Y, val_Y\n",
        "\n",
        "\"\"\"\n",
        "Returns: \n",
        "  train_loader: DataLoader used during training\n",
        "  val_loader: DataLoader used during validation\n",
        "  test_loader: DataLoader used during testing\n",
        "Input arguments\n",
        "  annotations_frame: dataframe where each image is associated to a label\n",
        "  labels: newly computed labels\n",
        "  batch_size: batch size used during training phase\n",
        "  img_root: directory containing the dataset images\n",
        "  test_batch_size: batch size used during validation and test phase\n",
        "\"\"\"\n",
        "def get_data(augmented_annotations_frame, labels, batch_size, img_root, test_batch_size=256):\n",
        "  # Get splitted data\n",
        "  X_train, X_val, y_train, y_val = split_training_data(root_dir=\"%s/train\" % (img_root), annotations_frame=augmented_annotations_frame, labels=labels)\n",
        "\n",
        "  # Load data for training and validation\n",
        "  training_data = PeopleTrainDataset(X=X_train,\n",
        "                                     Y=y_train,\n",
        "                                     transform=None)\n",
        "  \n",
        "  validation_data = PeopleTrainDataset(X=X_val,\n",
        "                                     Y=y_val,\n",
        "                                     transform=None)\n",
        "  \n",
        "  test_data = PeopleTestDataset(root_dir=\"%s/test\" % (img_root))\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True, num_workers=0)    \n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False, num_workers=0)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False, num_workers=0)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnndkhgI8oOb"
      },
      "source": [
        "# Logger for loss and accuracy at each step \n",
        "def log_values(writer, step, loss, accuracy, prefix):\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
        "  writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
        "\n",
        "def main(batch_size=256, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         epochs=5, \n",
        "         img_root='./dataset'):\n",
        "  \n",
        "  #Instantiate the SummaryWriter for Tensorboard visualization\n",
        "  writer = SummaryWriter(log_dir=\"runs/resnet50_5epoch\")\n",
        "\n",
        "  #Parse annotations \n",
        "  annotations_frame = pd.read_csv(\"./dataset/annotations_train.csv\")\n",
        "  \n",
        "  #Get labels for every training image\n",
        "  augmented_annotations_frame = convertAnnotationsFrame(annotations_frame, \"%s/train\" % (img_root))\n",
        "\n",
        "  #Get dictionary where each label is associated with a position in the one-hot encoding\n",
        "  labels = setupLabelsDict(augmented_annotations_frame)\n",
        "\n",
        "  # Instantiates dataloaders\n",
        "  train_loader, val_loader, test_loader = get_data(augmented_annotations_frame=augmented_annotations_frame, labels=labels, batch_size=batch_size, img_root=img_root)\n",
        "  \n",
        "  # Instantiates the model\n",
        "  net = initialize_resnet(num_classes=len(labels)).to(device)\n",
        "  \n",
        "  # Instantiates the optimizer\n",
        "  optimizer = get_optimizer(net, learning_rate)\n",
        "  \n",
        "  # Instantiates the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function, num_classes=len(labels))\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function, num_classes=len(labels))\n",
        "\n",
        "  log_values(writer, -1, train_loss, train_accuracy, \"Train\")\n",
        "  log_values(writer, -1, val_loss, val_accuracy, \"Validation\")\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function, num_classes=len(labels))\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function, num_classes=len(labels))\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    log_values(writer, e, train_loss, train_accuracy, \"Train\")\n",
        "    log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function, num_classes=len(labels))\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function, num_classes=len(labels))\n",
        "  log_values(writer, e, train_loss, train_accuracy, \"Train\")\n",
        "  log_values(writer, e, val_loss, val_accuracy, \"Validation\")\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # Closes the logger\n",
        "  writer.close()\n",
        "  \n",
        "  #output the trained network\n",
        "  return net"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqom_4Hz8oOc"
      },
      "source": [
        "# Free GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# clear runs\n",
        "#! rm -r runs\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=runs/resnet50_5epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppQx1-td8oOc"
      },
      "source": [
        "net = main()\n",
        "\n",
        "#save the trained model\n",
        "torch.save(net.state_dict(), \"./resnet50_5epoch.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpQAVSMaJTCy"
      },
      "source": [
        "## Make a zip containing the runs if need to save them\n",
        "#import shutil\n",
        "#shutil.make_archive('runs_23_08', 'zip', 'runs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD7gUlVZp7U9"
      },
      "source": [
        "## Save the model directly in Google Drive if needed for other tasks\n",
        "#torch.save(net.state_dict(), \"/content/drive/MyDrive/UNITN/5Â° anno/Deep Learning 2021/models/resnet50_5epoch\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}