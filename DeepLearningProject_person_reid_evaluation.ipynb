{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "DeepLearningProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "fb220023fd2800431fe98a81f852d0ad5fc6056086b92ba5142182fcc04964c6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject_person_reid_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Project - Person Re-identification task evaluation\n",
        "\n",
        "[https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject_person_reid_evaluation.ipynb](https://colab.research.google.com/github/dadebulba/DeepLearningProject/blob/main/DeepLearningProject_person_reid_evaluation.ipynb)"
      ],
      "metadata": {
        "id": "W6jMA8w_8oOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing from Google Drive the dataset.zip and extract into dataset folder, change the path with your dataset location"
      ],
      "metadata": {
        "id": "tM3c4xWtqiDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "!unzip \"/content/drive/MyDrive/UNITN/5Â° anno/Deep Learning 2021/dataset.zip\" -d dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "CaPQ91Z78oOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing necessary libraries"
      ],
      "metadata": {
        "id": "jg8kbiHv8oOL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "import pandas as pd\r\n",
        "from skimage import io, transform\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from torchvision import transforms, utils\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import os\r\n",
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from PIL import Image\r\n",
        "import random\r\n",
        "random.seed(10)\r\n",
        "# print cuda info\r\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\r\n",
        "print(f\"Cuda device count: {torch.cuda.device_count()}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda available: True\n",
            "Cuda device count: 1\n"
          ]
        }
      ],
      "metadata": {
        "id": "NUDHglK08oOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6bf7fb-dbbd-494d-96b3-a2f785f748d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Siamese Network\n",
        "This step is used to load the saved model and use it during evaluation "
      ],
      "metadata": {
        "id": "WwZMA17l8oOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "class Identity(nn.Module):\r\n",
        "  \"\"\"Identity layer to use into network\"\"\"\r\n",
        "  def __init__(self):\r\n",
        "      super(Identity, self).__init__()\r\n",
        "      \r\n",
        "  def forward(self, x):\r\n",
        "      return x\r\n",
        "\r\n",
        "class Siamese(nn.Module):\r\n",
        "  \"\"\"Siamese network using two resnet-50 as branches\"\"\"\r\n",
        "  \"\"\"\r\n",
        "  Args:\r\n",
        "    resnet: trained resnet-50\r\n",
        "  \"\"\"\r\n",
        "  def __init__(self, resnet):\r\n",
        "      super(Siamese, self).__init__()\r\n",
        "      self.resnet = resnet\r\n",
        "      self.resnet.fc = Identity()\r\n",
        "      self.linear = torch.nn.Sequential(\r\n",
        "        torch.nn.Linear(in_features=2048, out_features=1024),\r\n",
        "        torch.nn.Linear(in_features=1024, out_features=512),\r\n",
        "        torch.nn.Sigmoid()\r\n",
        "      )\r\n",
        "  \"\"\"\r\n",
        "  Returns: resulting tensor from input inference into one branch of siamese\r\n",
        "  Args:\r\n",
        "    x: input image\r\n",
        "  \"\"\"\r\n",
        "  def forward_one(self, x):\r\n",
        "      x = self.resnet(x)\r\n",
        "      x = x.view(x.size()[0], -1)\r\n",
        "      x = self.linear(x)\r\n",
        "      return x\r\n",
        "  \"\"\"\r\n",
        "  Returns: resulting tensors from input inference into siamese\r\n",
        "  Args:\r\n",
        "    x1: input image1\r\n",
        "    x2: input image2\r\n",
        "  \"\"\"\r\n",
        "  def forward(self, x1, x2):\r\n",
        "      out1 = self.forward_one(x1)\r\n",
        "      out2 = self.forward_one(x2)\r\n",
        "      return out1, out2\r\n",
        "\r\n",
        "'''\r\n",
        "Returns: fine tuned resnet-50\r\n",
        "Args:\r\n",
        "  num_classes: number of classes in the dataset.\r\n",
        "               This is equal to the number of output neurons.\r\n",
        "'''\r\n",
        "def initialize_resnet(num_classes):\r\n",
        "  #load pre-trained resnet\r\n",
        "  resnet = torchvision.models.resnet50(pretrained=True)\r\n",
        "  num_features = resnet.fc.in_features\r\n",
        "  resnet.fc = torch.nn.Sequential(\r\n",
        "    torch.nn.Linear(in_features=num_features, out_features=1024),\r\n",
        "    torch.nn.Linear(in_features=1024, out_features=512),\r\n",
        "    torch.nn.Linear(in_features=512, out_features=num_classes),\r\n",
        "    torch.nn.Sigmoid()\r\n",
        "  )\r\n",
        "\r\n",
        "  return resnet"
      ],
      "outputs": [],
      "metadata": {
        "id": "9__1oD_z8oOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "class PeopleValidationDataset(Dataset):\r\n",
        "    \"\"\"People training dataset containing tuple of images with corresponding similarity value.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, X1, X2, root_dir, transform):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            X1: first list of image names\r\n",
        "            X2: first list of image names\r\n",
        "            root_dir: folder where to find the images to load\r\n",
        "            transform (optional): Optional transform to be applied on a sample.\r\n",
        "        \"\"\"\r\n",
        "        self.transform = transform\r\n",
        "        self.X1 = X1\r\n",
        "        self.X2 = X2\r\n",
        "        self.root_dir = root_dir\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X1)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        img_name1 = self.X1[idx]\r\n",
        "        img_name2 = self.X2[idx]\r\n",
        "\r\n",
        "        image1 = Image.open(\"%s/%s\" % (self.root_dir, img_name1))\r\n",
        "        image2 = Image.open(\"%s/%s\" % (self.root_dir, img_name2))\r\n",
        "        if self.transform != None:\r\n",
        "          image1 = self.transform(image1)\r\n",
        "          image2 = self.transform(image2)\r\n",
        "        else:\r\n",
        "          image1 = T.ToTensor()(image1)\r\n",
        "          image2 = T.ToTensor()(image2)\r\n",
        "        image1 = F.interpolate(image1, size=128)  \r\n",
        "        image2 = F.interpolate(image2, size=128)  \r\n",
        "\r\n",
        "        sample = (image1, img_name1, image2, img_name2)\r\n",
        "        return sample"
      ],
      "outputs": [],
      "metadata": {
        "id": "fINP--L08oOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "\"\"\"\r\n",
        "Returns: \r\n",
        "  train_X1: first list of inputs used during training\r\n",
        "  train_X2: second list of inputs used during training\r\n",
        "  train_Y: similarity values for training inputs\r\n",
        "  val_X1: first list of inputs used during validation\r\n",
        "  val_X2: second list of inputs used during validation\r\n",
        "  val_Y: similarity values for validation inputs\r\n",
        "Input arguments\r\n",
        "  root_dir: directory containing the dataset images\r\n",
        "  val_rate (optional): percentage of images to put into validation set\r\n",
        "\"\"\"\r\n",
        "def split_training_data(root_dir, val_rate=0.03):\r\n",
        "  full_dataset_X1 = []\r\n",
        "  full_dataset_X2 = []\r\n",
        "  full_dataset_Y = []\r\n",
        "  img_files = [f for f in listdir(root_dir)]\r\n",
        "  img_files.sort()\r\n",
        "  if (len(img_files) % 2 != 0):\r\n",
        "    img_files = img_files[:-1]\r\n",
        "\r\n",
        "  for idx, img in enumerate(img_files):\r\n",
        "    if(idx % 2 == 0):\r\n",
        "      full_dataset_X1.append(img)\r\n",
        "    if(idx % 2 != 0):\r\n",
        "      full_dataset_X2.append(img)\r\n",
        "      if (int(full_dataset_X1[-1].split(\"_\")[0]) == int(full_dataset_X2[-1].split(\"_\")[0])):\r\n",
        "        full_dataset_Y.append(torch.tensor(1))\r\n",
        "      else:\r\n",
        "        full_dataset_Y.append(torch.tensor(0))\r\n",
        "\r\n",
        "  val_X1 = []\r\n",
        "  val_X2 = []\r\n",
        "  val_Y = []\r\n",
        "  train_X1 = []\r\n",
        "  train_X2 = []\r\n",
        "  train_Y = []\r\n",
        "  for idx, _ in enumerate(full_dataset_Y):\r\n",
        "    if (idx <= len(full_dataset_Y)*val_rate):\r\n",
        "      val_X1.append(full_dataset_X1[idx])\r\n",
        "      val_X2.append(full_dataset_X2[idx])\r\n",
        "      val_Y.append(full_dataset_Y[idx])\r\n",
        "    else:\r\n",
        "      train_X1.append(full_dataset_X1[idx])\r\n",
        "      train_X2.append(full_dataset_X2[idx])\r\n",
        "      train_Y.append(full_dataset_Y[idx])\r\n",
        "\r\n",
        "  return train_X1, train_X2, train_Y, val_X1, val_X2, val_Y\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Returns: \r\n",
        "  val_map: a map with image id as key and list of associated images as value\r\n",
        "Input arguments\r\n",
        "  val_X1: first list of inputs used during validation\r\n",
        "  val_X2: second list of inputs used during validation\r\n",
        "\"\"\"\r\n",
        "def build_val_map(val_X1, val_X2):\r\n",
        "  val_map = {}\r\n",
        "  for i in range(len(val_X1)):\r\n",
        "    if int(val_X1[i].split(\"_\")[0]) not in val_map:\r\n",
        "      val_map[int(val_X1[i].split(\"_\")[0])] = []\r\n",
        "    if int(val_X2[i].split(\"_\")[0]) not in val_map:\r\n",
        "      val_map[int(val_X2[i].split(\"_\")[0])] = []\r\n",
        "    val_map[int(val_X1[i].split(\"_\")[0])].append(val_X1[i])\r\n",
        "    val_map[int(val_X2[i].split(\"_\")[0])].append(val_X2[i])\r\n",
        "  for key in val_map:\r\n",
        "    val_map[key] = list(set(val_map[key]))\r\n",
        "  return val_map\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Returns: \r\n",
        "  val_loader: DataLoader used during evaluation\r\n",
        "Input arguments\r\n",
        "  img_root: directory containing the dataset images\r\n",
        "\"\"\"\r\n",
        "def get_data(img_root):\r\n",
        "\r\n",
        "  # Get splitted data\r\n",
        "  train_X1, train_X2, train_Y, val_X1, val_X2, val_Y = split_training_data(root_dir=img_root)\r\n",
        "  val_map = build_val_map(val_X1, val_X2)\r\n",
        "  val_merged =  list(set(val_X1 + val_X2))\r\n",
        "  print(len(val_map), len(val_merged))\r\n",
        "\r\n",
        "  val_X1 = []\r\n",
        "  for key in val_map:\r\n",
        "    val_X1 = val_X1 + [val_map[key][0] for i in range(len(val_merged))]\r\n",
        "  val_X2 = val_merged*len(val_map.keys())\r\n",
        "  print(len(val_X1), len(val_X2))\r\n",
        "  validation_data = PeopleValidationDataset(X1=val_X1,\r\n",
        "                                     X2=val_X2,\r\n",
        "                                     root_dir=img_root,\r\n",
        "                                     transform=None)\r\n",
        "\r\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, shuffle=False, num_workers=0) #before num_workers=4\r\n",
        "  \r\n",
        "  return val_loader"
      ],
      "outputs": [],
      "metadata": {
        "id": "L_h1U46GZahd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "def test(net, val_loader, threshold , device='cuda:0'):\r\n",
        "  predictions = {}\r\n",
        "  ground_truth = {}\r\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\r\n",
        "  with torch.no_grad():\r\n",
        "    dataiter = iter(val_loader)\r\n",
        "    i = 0\r\n",
        "    while True:\r\n",
        "      try:\r\n",
        "        x0, name0, x1, name1 = next(dataiter)\r\n",
        "        i+=1\r\n",
        "        if i % 5000 == 0:\r\n",
        "          print(\"Passed images:\", i)\r\n",
        "      except:\r\n",
        "        break\r\n",
        "      x0 = x0.to('cuda:0')\r\n",
        "      x1 = x1.to('cuda:0')\r\n",
        "      concatenated = torch.cat((x0,x1),0)\r\n",
        "\r\n",
        "      # Forward pass\r\n",
        "      output1, output2 = net.forward(x0, x1)\r\n",
        "      euclidean_distance = F.pairwise_distance(output1, output2)\r\n",
        "\r\n",
        "      if name0 not in ground_truth:\r\n",
        "        ground_truth[name0] = []\r\n",
        "      if (name0[0].split(\"_\")[0] == name1[0].split(\"_\")[0]):\r\n",
        "        ground_truth[name0].append(name1)\r\n",
        "\r\n",
        "      if name0 not in predictions:\r\n",
        "        predictions[name0] = []\r\n",
        "      if euclidean_distance.item() < threshold:\r\n",
        "        predictions[name0].append(name1)\r\n",
        "        \r\n",
        "\r\n",
        "  return predictions, ground_truth"
      ],
      "outputs": [],
      "metadata": {
        "id": "m2f-sc08Zd_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "def evaluate_map(predictions, ground_truth):\r\n",
        "      '''\r\n",
        "      Computes the mAP (https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173) of the predictions with respect to the given ground truth\r\n",
        "      In person reidentification mAP refers to the mean of the AP over all queries.\r\n",
        "      The AP for a query is the area under the precision-recall curve obtained from the list of predictions considering the\r\n",
        "      ground truth elements as positives and the other ones as negatives\r\n",
        "\r\n",
        "      :param predictions: dictionary from query filename to list of test image filenames associated with the query ordered\r\n",
        "                          from the most to the least confident prediction.\r\n",
        "                          Represents the predictions to be evaluated.\r\n",
        "      :param ground_truth: dictionary from query filename to set of test image filenames associated with the query\r\n",
        "                            Represents the ground truth on which to evaluate predictions.\r\n",
        "\r\n",
        "      :return:\r\n",
        "      '''\r\n",
        "\r\n",
        "      m_ap = 0.0\r\n",
        "      for current_ground_truth_query, current_ground_truth_query_set in ground_truth.items():\r\n",
        "\r\n",
        "          # No predictions were performed for the current query, AP = 0\r\n",
        "          if not current_ground_truth_query in predictions:\r\n",
        "              continue\r\n",
        "\r\n",
        "          current_ap = 0.0  # The area under the curve for the current sample\r\n",
        "          current_predictions_list = predictions[current_ground_truth_query]\r\n",
        "\r\n",
        "          # Recall increments of this quantity each time a new correct prediction is encountered in the prediction list\r\n",
        "          delta_recall = 1.0 / len(current_ground_truth_query_set)\r\n",
        "\r\n",
        "          # Goes through the list of predictions\r\n",
        "          encountered_positives = 0\r\n",
        "          for idx, current_prediction in enumerate(current_predictions_list):\r\n",
        "              # Each time a positive is encountered, compute the current precition and the area under the curve\r\n",
        "              # since the last positive\r\n",
        "              if current_prediction in current_ground_truth_query_set:\r\n",
        "                  encountered_positives += 1\r\n",
        "                  current_precision = encountered_positives / (idx + 1)\r\n",
        "                  current_ap += current_precision * delta_recall\r\n",
        "\r\n",
        "          m_ap += current_ap\r\n",
        "\r\n",
        "      # Compute mean over all queries\r\n",
        "      m_ap /= len(ground_truth)\r\n",
        "\r\n",
        "      return m_ap"
      ],
      "outputs": [],
      "metadata": {
        "id": "5O965dfHZgBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "# Logger for loss and accuracy at each step \r\n",
        "def log_values(writer, step, loss, prefix):\r\n",
        "  writer.add_scalar(f\"{prefix}/loss\", loss, step)\r\n",
        "\r\n",
        "def main(device='cuda:0', \r\n",
        "         img_root='./dataset',\r\n",
        "         model_root=\"G:/Il mio Drive/UNITN/5Â° anno/Deep Learning 2021/models/siamese_15epoch_net_reid_resnet50_5epoch\",\r\n",
        "         step=0.01,\r\n",
        "         num_step=20):\r\n",
        "\r\n",
        "  writer = SummaryWriter(log_dir=\"runs/exp4\")\r\n",
        "\r\n",
        "  # Get dataloader containing data to evaluate\r\n",
        "  val_loader = get_data(img_root=\"%s/train\" % (img_root))\r\n",
        "\r\n",
        "  # Instantiates the model\r\n",
        "  net = initialize_resnet(num_classes=56) # taken from previous trained resnet last layer number of neurons\r\n",
        "  net = Siamese(net)\r\n",
        "  net.load_state_dict(torch.load(model_root))\r\n",
        "  net.to(device)\r\n",
        "\r\n",
        "  prev_mAP = 0\r\n",
        "  # Starting from a threshold of 0 increase by step value for num_step, will stop at end of num_step or when mAP degrades\r\n",
        "  for i in [step*(j+1) for j in range(num_step)]:\r\n",
        "    predictions, ground_truth = test(net, val_loader, i)    \r\n",
        "    mAP = evaluate_map(predictions, ground_truth)\r\n",
        "    print(\"mAP for threshold %s is %s\" % (i, mAP))\r\n",
        "    if mAP < prev_mAP:\r\n",
        "      print(\"Performance degradation, breaking...\", mAP, prev_mAP)\r\n",
        "      break\r\n",
        "    prev_mAP = mAP"
      ],
      "outputs": [],
      "metadata": {
        "id": "QnndkhgI8oOb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24 390\n",
            "9360 9360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "D:\\Users\\dadeb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed images: 5000\n",
            "mAP for threshold 0.01 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.02 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.03 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.04 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.05 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.06 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.07 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.08 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.09 is 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.1 is 0.0955231573622378\n",
            "Performance degradation, breaking... 0.0955231573622378 0.09647012705920749\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.11 is 0.0955231573622378\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.12 is 0.09378704625112667\n",
            "Performance degradation, breaking... 0.09378704625112667 0.0955231573622378\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.13 is 0.09320834254742298\n",
            "Performance degradation, breaking... 0.09320834254742298 0.09378704625112667\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.14 is 0.09226137285045329\n",
            "Performance degradation, breaking... 0.09226137285045329 0.09320834254742298\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.15 is 0.0903674334565139\n",
            "Performance degradation, breaking... 0.0903674334565139 0.09226137285045329\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.16 is 0.08776326678984724\n",
            "Performance degradation, breaking... 0.08776326678984724 0.0903674334565139\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.17 is 0.08776326678984724\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.18 is 0.08684260180668224\n",
            "Performance degradation, breaking... 0.08684260180668224 0.08776326678984724\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.19 is 0.08684260180668224\n",
            "Passed images: 5000\n",
            "mAP for threshold 0.2 is 0.08250232402890448\n",
            "Performance degradation, breaking... 0.08250232402890448 0.08684260180668224\n"
          ]
        }
      ],
      "metadata": {
        "id": "ppQx1-td8oOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e5ab16-d48f-4495-fbc3-822017cda749"
      }
    }
  ]
}